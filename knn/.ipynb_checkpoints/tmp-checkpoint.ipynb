{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e691e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & helpers\n",
    "\n",
    "import os, json, math, random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# ---------- config ----------\n",
    "MODEL_PATH = \"/root/autodl-tmp/EasyR1/ckpt/bge-m3\"\n",
    "DEVICE = \"cuda\"  # or \"cpu\"\n",
    "BATCH_SIZE = 8\n",
    "NORMALIZE_EMBEDDINGS = True\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# ---------- I/O ----------\n",
    "def read_json_any(path: str) -> List[Dict[str, Any]]:\n",
    "    p = Path(path)\n",
    "    if p.suffix.lower() == \".jsonl\":\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return [json.loads(l) for l in f if l.strip()]\n",
    "    elif p.suffix.lower() == \".json\":\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        if isinstance(data, dict):\n",
    "            for k in [\"data\", \"items\", \"rows\"]:\n",
    "                if k in data and isinstance(data[k], list):\n",
    "                    return data[k]\n",
    "            return [data]\n",
    "    else:\n",
    "        raise ValueError(f\"unsupported file type: {p.suffix}\")\n",
    "\n",
    "def get_by_dotted_key(obj: Dict[str, Any], dotted: str, default=None):\n",
    "    cur = obj\n",
    "    for part in dotted.split(\".\"):\n",
    "        if not isinstance(cur, dict) or part not in cur:\n",
    "            return default\n",
    "        cur = cur[part]\n",
    "    return cur\n",
    "\n",
    "def build_text(row: Dict[str, Any]) -> str:\n",
    "    q = row.get(\"question\", \"\") or \"\"\n",
    "    a = row.get(\"answer\", \"\") or \"\"\n",
    "    return f\"Q: {q}\\nA: {a}\".strip()\n",
    "\n",
    "def build_text_from_row(row: Dict[str, Any], source: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    将不同来源/格式的样本转换为可用于嵌入的统一文本。\n",
    "    source ∈ {\"qa\", \"mcq\", \"cs\", \"paragraph\", \"tfq\"}\n",
    "    \"\"\"\n",
    "    if source == \"qa\":\n",
    "        # 复用你已有的 QA 组装逻辑\n",
    "        return build_text(row)\n",
    "\n",
    "    if source == \"mcq\":\n",
    "        # 直接使用 fine_mcq 字段原文\n",
    "        return row.get(\"fine_mcq\")\n",
    "\n",
    "    if source == \"cs\":\n",
    "        # 转换为 C: <category>\\nS: <summarised_text>\n",
    "        c = row.get(\"category\")\n",
    "        s = row.get(\"summarised_text\")\n",
    "        if c is None and \"summarised_text\" not in row:\n",
    "            # 有些数据可能嵌套或字段名不同，可按需扩展 get_by_dotted_key\n",
    "            c = get_by_dotted_key(row, \"category\", \"\")\n",
    "            s = get_by_dotted_key(row, \"summarised_text\", \"\")\n",
    "        if c is None or s is None:\n",
    "            return None\n",
    "        return f\"C: {c}\\nS: {s}\"\n",
    "\n",
    "    if source == \"paragraph\":\n",
    "        # 直接取 paragraph 字段\n",
    "        return row.get(\"paragraph\")\n",
    "\n",
    "    if source == \"tfq\":\n",
    "        # 直接取 tfq_part（里面已含“...  Answer: True/False”）\n",
    "        return row.get(\"tfq_part\")\n",
    "    print(\"Warning!!!!\")\n",
    "    return None\n",
    "# ---------- embedding ----------\n",
    "_model_cache = {}\n",
    "def get_embedder(model_path: str = MODEL_PATH, device: str = DEVICE):\n",
    "    key = (model_path, device)\n",
    "    if key not in _model_cache:\n",
    "        _model_cache[key] = SentenceTransformer(model_path, device=device)\n",
    "    return _model_cache[key]\n",
    "\n",
    "def encode_texts(texts: List[str],\n",
    "                 model_path: str = MODEL_PATH,\n",
    "                 device: str = DEVICE,\n",
    "                 batch_size: int = BATCH_SIZE,\n",
    "                 normalize_embeddings: bool = NORMALIZE_EMBEDDINGS) -> np.ndarray:\n",
    "    model = get_embedder(model_path, device)\n",
    "    emb = model.encode(\n",
    "        texts, batch_size=batch_size, show_progress_bar=True,\n",
    "        convert_to_numpy=True, normalize_embeddings=normalize_embeddings\n",
    "    )\n",
    "    return emb.astype(np.float32)\n",
    "\n",
    "# ---------- Bayes 评分所需 ----------\n",
    "class DiscreteIndexer:\n",
    "    \"\"\"把离散分值（如 1..5）映射到 0..C-1 的索引，并支持反向期望计算\"\"\"\n",
    "    def __init__(self, values: np.ndarray):\n",
    "        vals = np.sort(np.unique(values.astype(int)))\n",
    "        self.values = vals\n",
    "        self.val2idx = {int(v): i for i, v in enumerate(vals)}\n",
    "\n",
    "    @property\n",
    "    def n_classes(self) -> int:\n",
    "        return len(self.values)\n",
    "\n",
    "    def to_index(self, arr: np.ndarray) -> np.ndarray:\n",
    "        out = np.zeros_like(arr, dtype=int)\n",
    "        for i, v in enumerate(arr.astype(int)):\n",
    "            out[i] = self.val2idx[int(v)]\n",
    "        return out\n",
    "\n",
    "    def to_value_expectation(self, post: np.ndarray) -> float:\n",
    "        return float(np.dot(self.values.astype(float), post))\n",
    "\n",
    "def estimate_T_from_B(B_emb: np.ndarray,\n",
    "                      B_scores_idx: np.ndarray,\n",
    "                      n_classes: int,\n",
    "                      k_t: int = 2,\n",
    "                      metric: str = \"cosine\",\n",
    "                      smoothing: float = 1e-2) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    在历史库 B 内部用 k_t-NN 共现估计 T（近似 P(obs=j | true=i)）与先验 p\n",
    "    \"\"\"\n",
    "    nn = NearestNeighbors(n_neighbors=k_t+1, metric=metric)\n",
    "    nn.fit(B_emb)\n",
    "    _, idx = nn.kneighbors(B_emb, return_distance=True)\n",
    "\n",
    "    C = np.full((n_classes, n_classes), smoothing, dtype=float)\n",
    "    for center, neighs in enumerate(idx):\n",
    "        ci = int(B_scores_idx[center])\n",
    "        for nb in neighs[1:1+k_t]:  # 跳过自身\n",
    "            cj = int(B_scores_idx[nb])\n",
    "            C[ci, cj] += 1.0\n",
    "\n",
    "    T = C / C.sum(axis=1, keepdims=True)\n",
    "    counts = np.bincount(B_scores_idx, minlength=n_classes).astype(float) + 1e-8\n",
    "    p = counts / counts.sum()\n",
    "    return T, p\n",
    "\n",
    "def bayes_from_hist(neigh_hist: np.ndarray, T: np.ndarray, p: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"log p_i + sum_j hist_j * log T_{i,j}  →  softmax → 后验\"\"\"\n",
    "    logp = np.log(p + 1e-12)\n",
    "    logT = np.log(T + 1e-12)\n",
    "    C = T.shape[0]\n",
    "    post_log = np.zeros(C, dtype=float)\n",
    "    for i in range(C):\n",
    "        post_log[i] = logp[i] + float(np.dot(neigh_hist, logT[i]))\n",
    "    m = post_log.max()\n",
    "    post = np.exp(post_log - m); post /= post.sum()\n",
    "    entropy = float(-np.sum(post * np.log(post + 1e-12)))\n",
    "    return post, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f01280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] B size = 22037; score classes = [0 2 3 4 5]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1dc19c677b404e83e31f47e7ec2dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1378 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] assets saved to: /root/autodl-tmp/EasyR1/knn_bayes_lib\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: build & persist library assets\n",
    "\n",
    "DATA_PATH = \"/root/autodl-tmp/EasyR1/examples/scored_raw2qa.jsonl\"\n",
    "OUT_DIR = \"/root/autodl-tmp/EasyR1/knn_bayes_lib\"              # 资产目录（可自定义）\n",
    "METRIC = \"cosine\"                        # 最近邻度量\n",
    "K_T = 2                                  # 估计 T 的 B 内部近邻数\n",
    "K_FOR_ONLINE = 16                        # 在线检索时默认 K\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) 读取并提取文本 + 历史评分（Overall_compressed）\n",
    "rows = read_json_any(DATA_PATH)\n",
    "B_texts, B_scores, B_uids = [], [], []\n",
    "for i, r in enumerate(rows):\n",
    "    sc = get_by_dotted_key(r, \"rating_detail.Overall_compressed\", None)\n",
    "    if sc is None:\n",
    "        continue\n",
    "    B_texts.append(build_text(r))\n",
    "    B_scores.append(int(sc))\n",
    "    B_uids.append(r.get(\"id\", i))\n",
    "\n",
    "B_scores = np.array(B_scores, dtype=int)\n",
    "print(f\"[INFO] B size = {len(B_scores)}; score classes = {np.unique(B_scores)}\")\n",
    "\n",
    "# 2) 嵌入\n",
    "B_emb = encode_texts(B_texts)  # shape [NB, d]\n",
    "\n",
    "# 3) 类别映射 + 估计 T, p\n",
    "class_values = np.sort(np.unique(B_scores))\n",
    "indexer = DiscreteIndexer(class_values)\n",
    "B_idx = indexer.to_index(B_scores)\n",
    "T, p = estimate_T_from_B(B_emb, B_idx, indexer.n_classes, k_t=K_T, metric=METRIC)\n",
    "\n",
    "# 4) 拟合 KNN 索引（用于在线检索）\n",
    "nn = NearestNeighbors(n_neighbors=K_FOR_ONLINE, metric=METRIC)\n",
    "nn.fit(B_emb)\n",
    "\n",
    "# 5) 保存资产\n",
    "np.save(Path(OUT_DIR) / \"B_emb.npy\", B_emb)\n",
    "np.save(Path(OUT_DIR) / \"B_scores.npy\", B_scores)\n",
    "with open(Path(OUT_DIR) / \"B_uids.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for u in B_uids:\n",
    "        f.write(json.dumps({\"uid\": u}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "meta = {\n",
    "    \"model_path\": MODEL_PATH,\n",
    "    \"device_hint\": DEVICE,\n",
    "    \"normalize_embeddings\": NORMALIZE_EMBEDDINGS,\n",
    "    \"metric\": METRIC,\n",
    "    \"k_t\": K_T,\n",
    "    \"k_default\": K_FOR_ONLINE,\n",
    "    \"class_values\": class_values.tolist(),\n",
    "    \"T\": T.tolist(),\n",
    "    \"p\": p.tolist(),\n",
    "}\n",
    "with open(Path(OUT_DIR) / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "joblib.dump(nn, Path(OUT_DIR) / \"nn_index.pkl\")\n",
    "print(f\"[OK] assets saved to: {OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3ff1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: load assets & define online scoring\n",
    "\n",
    "LIB_DIR = \"./knn_bayes_lib\"  # 与上一步 OUT_DIR 保持一致\n",
    "assert Path(LIB_DIR).exists(), \"library dir not found\"\n",
    "\n",
    "# 1) 加载资产\n",
    "B_emb = np.load(Path(LIB_DIR) / \"B_emb.npy\")\n",
    "B_scores = np.load(Path(LIB_DIR) / \"B_scores.npy\")\n",
    "with open(Path(LIB_DIR) / \"meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "class_values = np.array(meta[\"class_values\"], dtype=int)\n",
    "indexer = DiscreteIndexer(class_values)\n",
    "B_idx = indexer.to_index(B_scores)\n",
    "\n",
    "T = np.array(meta[\"T\"], dtype=float)\n",
    "p = np.array(meta[\"p\"], dtype=float)\n",
    "METRIC = meta[\"metric\"]\n",
    "K_DEFAULT = int(meta.get(\"k_default\", 16))\n",
    "\n",
    "# 加载 KNN 索引（如不存在可临时拟合）\n",
    "if Path(LIB_DIR, \"nn_index.pkl\").exists():\n",
    "    nn = joblib.load(Path(LIB_DIR, \"nn_index.pkl\"))\n",
    "else:\n",
    "    nn = NearestNeighbors(n_neighbors=K_DEFAULT, metric=METRIC)\n",
    "    nn.fit(B_emb)\n",
    "\n",
    "# 2) 在线评分函数\n",
    "def score_with_bayes(new_samples: List[Dict[str, Any]],\n",
    "                     k: int = K_DEFAULT,\n",
    "                     return_neighbors: int = 0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    new_samples: 每个元素至少包含 title/question/answer 字段之一；或你自己拼好 text 放在 'text'\n",
    "    k: 检索近邻数\n",
    "    return_neighbors: 若 >0 则返回每条的前 n 个邻居（uid/score/相似度）\n",
    "    \"\"\"\n",
    "    # 组装文本\n",
    "    texts = []\n",
    "    for r in new_samples:\n",
    "        if \"text\" in r and r[\"text\"]:\n",
    "            texts.append(r[\"text\"])\n",
    "        else:\n",
    "            texts.append(build_text(r))\n",
    "\n",
    "    # 向量化（使用与库一致的模型/规范化）\n",
    "    X = encode_texts(texts)\n",
    "\n",
    "    # 检索\n",
    "    dists, idx = nn.kneighbors(X, n_neighbors=k, return_distance=True)\n",
    "\n",
    "    # 对每条样本做直方图 → Bayes 后验\n",
    "    results = []\n",
    "    for i, (row_d, row_i) in enumerate(zip(dists, idx)):\n",
    "        neigh_cls_idx = B_idx[row_i]\n",
    "        hist = np.bincount(neigh_cls_idx, minlength=indexer.n_classes).astype(float)\n",
    "        post, H = bayes_from_hist(hist, T, p)\n",
    "        # 离散预测（后验众数） & 连续期望\n",
    "        pred_mode = int(indexer.values[np.argmax(post)])\n",
    "        pred_exp = float(indexer.to_value_expectation(post))\n",
    "\n",
    "        rec = {\n",
    "            \"sample_id\": i,\n",
    "            \"bayes_score\": pred_mode,     # 离散分（推荐用于业务一致性）\n",
    "            \"bayes_exp\": pred_exp,        # 连续期望（可做细粒度统计）\n",
    "            \"bayes_entropy\": H,           # 不确定性\n",
    "        }\n",
    "\n",
    "        if return_neighbors > 0:\n",
    "            # 近邻相似度（cosine 距离下，用 1 - d 近似相似度）\n",
    "            sims = 1.0 - row_d\n",
    "            topn = min(return_neighbors, len(row_i))\n",
    "            neigh_info = []\n",
    "            for j in range(topn):\n",
    "                neigh_info.append({\n",
    "                    \"lib_index\": int(row_i[j]),\n",
    "                    \"lib_score\": int(B_scores[row_i[j]]),\n",
    "                    \"sim\": float(sims[j])\n",
    "                })\n",
    "            rec[\"neighbors\"] = neigh_info\n",
    "\n",
    "        results.append(rec)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e9eb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0065e46d2a45238b815f406c4663ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>bayes_score</th>\n",
       "      <th>bayes_exp</th>\n",
       "      <th>bayes_entropy</th>\n",
       "      <th>neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.979516</td>\n",
       "      <td>0.152872</td>\n",
       "      <td>[{'lib_index': 22036, 'lib_score': 4, 'sim': 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id  bayes_score  bayes_exp  bayes_entropy  \\\n",
       "0          0            4   3.979516       0.152872   \n",
       "\n",
       "                                           neighbors  \n",
       "0  [{'lib_index': 22036, 'lib_score': 4, 'sim': 0...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: example usage\n",
    "\n",
    "new_samples = [\n",
    "    {\n",
    "        \"title\": \"Sibley's and James Store Historic District\",\n",
    "        \"question\": \"How does the architectural connection between the Sibley Brothers General Store and the Old Thomas James Store reflect historical continuity?\",\n",
    "        \"answer\": \"The hyphen linking the 1899 Sibley Brothers store and the c.1810 Old Thomas James Store manifests continuity in Virginia’s small-town commerce...\"\n",
    "    },\n",
    "    # 你可以继续追加更多样本；也可以直接传 {\"text\": \"...\"} 形式\n",
    "]\n",
    "\n",
    "df_pred = score_with_bayes(new_samples, k=16, return_neighbors=5)\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f3108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 9332 samples to /root/autodl-tmp/EasyR1/sft/data/qa/merged_train_qa_noisy_30.json\n",
      "Saved 21776 samples to /root/autodl-tmp/EasyR1/sft/data/qa/merged_train_qa_noisy_70.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "in_path = \"/root/autodl-tmp/EasyR1/sft/data/qa/merged_train_qa_noisy.json\"\n",
    "out_dir = \"/root/autodl-tmp/EasyR1/sft/data/qa\"\n",
    "\n",
    "# 读入原始数据\n",
    "with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 计算切分点\n",
    "ratio = int(len(data) * 0.3)\n",
    "\n",
    "# 前 30%\n",
    "data_30 = data[:ratio]\n",
    "# 剩下 70%\n",
    "data_70 = data[ratio:]\n",
    "\n",
    "# 写出文件\n",
    "out_path_30 = os.path.join(out_dir, \"merged_train_qa_noisy_30.json\")\n",
    "out_path_70 = os.path.join(out_dir, \"merged_train_qa_noisy_70.json\")\n",
    "\n",
    "with open(out_path_30, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_30, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(out_path_70, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_70, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(data_30)} samples to {out_path_30}\")\n",
    "print(f\"Saved {len(data_70)} samples to {out_path_70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b1379a-b759-48a3-95b9-f186ab9aa61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总样本数: 21776\n",
      "训练集: 19599 -> /root/autodl-tmp/EasyR1/sft/data/qa/merged_train_qa_noisy_70.train.json\n",
      "测试集: 2177 -> /root/autodl-tmp/EasyR1/sft/data/qa/merged_train_qa_noisy_70.test.json\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== 参数区 ====\n",
    "src_path = Path(\"/root/autodl-tmp/EasyR1/sft/data/qa/merged_train_qa_noisy_70.json\")\n",
    "test_ratio = 0.1\n",
    "seed = 42\n",
    "drop_instruction = True   # 如果要删除 instruction 字段，就设为 True\n",
    "# =================\n",
    "\n",
    "# 读取数据\n",
    "with src_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 合并 instruction -> input\n",
    "processed = []\n",
    "for item in data:\n",
    "    instr = (item.get(\"instruction\") or \"\").strip()\n",
    "    inp   = (item.get(\"input\") or \"\").strip()\n",
    "    if instr and inp:\n",
    "        new_input = f\"{instr}\\n\\n{inp}\"\n",
    "    else:\n",
    "        new_input = instr or inp\n",
    "    item[\"input\"] = new_input\n",
    "    if drop_instruction:\n",
    "        item.pop(\"instruction\", None)\n",
    "    processed.append(item)\n",
    "\n",
    "# 划分训练/测试\n",
    "rng = random.Random(seed)\n",
    "rng.shuffle(processed)\n",
    "n = len(processed)\n",
    "n_test = max(1, int(n * test_ratio))\n",
    "test_set = processed[:n_test]\n",
    "train_set = processed[n_test:]\n",
    "\n",
    "# 输出文件路径\n",
    "out_dir = src_path.parent\n",
    "stem = src_path.stem\n",
    "train_out = out_dir / f\"{stem}.train.json\"\n",
    "test_out  = out_dir / f\"{stem}.test.json\"\n",
    "\n",
    "# 保存\n",
    "with train_out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_set, f, ensure_ascii=False, indent=2)\n",
    "with test_out.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_set, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"总样本数: {n}\")\n",
    "print(f\"训练集: {len(train_set)} -> {train_out}\")\n",
    "print(f\"测试集: {len(test_set)} -> {test_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba1e45-f67b-40eb-9bd7-2205c725b7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
