# -*- coding: utf-8 -*-
import os
import re
import json
import torch
from dataclasses import dataclass
from typing import Any, Dict, Optional, Union

from sentence_transformers import SentenceTransformer
import regex
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    BitsAndBytesConfig,
)

JSON_RE = regex.compile(r"\{(?:[^{}]|(?R))*\}", re.DOTALL)
FORMAT_RE = re.compile(
    r"^\s*<think>.*?</think>\s*<answer>.*?</answer>\s*$",
    re.DOTALL | re.IGNORECASE
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ globals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_alignment_clf = None               # torch.nn.Module (MLP)
_embedding_model = None             # SentenceTransformer
_rater = None                       # RewardModelScorer instance
_weights = None                     # RewardWeights
_initialized = False                # flag

_default_quality_threshold = 4      # å·²ä¸å†ä½¿ç”¨é˜ˆå€¼ï¼Œå°†ä½¿ç”¨è¿ç»­åˆ†æ•°ï¼Œä½†ä¿ç•™å˜é‡ä¸å½±å“

import torch
from torch import nn

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MLP å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class MLPClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=256):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, 3)
        )
    def forward(self, x):
        return self.model(x)

def get_alignment_classifier(ckpt_path: Union[str, bytes]) -> MLPClassifier:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    ckpt = torch.load(ckpt_path, map_location=device)

    if "input_dim" not in ckpt or "model_state_dict" not in ckpt:
        raise ValueError("Checkpoint must contain 'input_dim' and 'model_state_dict'.")

    input_dim = ckpt["input_dim"]
    state_dict = ckpt["model_state_dict"]

    model = MLPClassifier(input_dim=input_dim).to(device)
    model.load_state_dict(state_dict)
    model.eval()

    return model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def strip_tag_spaces(s: str) -> str:
    return re.sub(r"\s*(<|>|/)\s*", r"\1", s)

def compress_overall_10_to_5(score_10: int) -> int:
    if score_10 <= 4:
        return 0
    elif score_10 >= 9:
        return 5
    else:
        return score_10 - 4

@dataclass
class RewardWeights:
    quality: float = 0.6
    alignment: float = 0.3
    format: float = 0.1

    def normalize(self) -> "RewardWeights":
        s = self.quality + self.alignment + self.format
        if s <= 0:
            raise ValueError("Sum of weights must be > 0.")
        return RewardWeights(
            self.quality / s, self.alignment / s, self.format / s
        )

def extract_answer_content(response: str) -> str:
    match = re.search(r"<answer>(.*?)</answer>", response, re.DOTALL)
    if match:
        return match.group(1).strip()
    return response.strip()

def format_reward(response: str) -> float:
    return 1.0 if re.fullmatch(FORMAT_RE, response or "") else 0.0

def alignment_reward(
    pair: str,
    model: torch.nn.Module,
    class_to_score: Dict[int, float] = {0: 0.0, 1: 0.2, 2: 1.0},
) -> float:
    # ä¾èµ–å…¨å±€ _embedding_model
    features = _embedding_model.encode([pair], normalize_embeddings=True)[0]
    input_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)

    with torch.no_grad():
        logits = model(input_tensor)
        probs = torch.softmax(logits, dim=-1)[0]
        pred_label = int(torch.argmax(probs).item())  # 0/1/2

    base = class_to_score.get(pred_label, 0.0)
    return base

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Reward Model (DeBERTa) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RewardModelScorer:
    """
    ä½¿ç”¨ OpenAssistant/reward-model-deberta-v3-large-v2 ä½œä¸ºè´¨é‡è¯„åˆ†å™¨ï¼š
    - è¾“å…¥: prompt ä¸ answerï¼ˆè‹¥æ—  promptï¼Œåˆ™ä»…ç”¨ answerï¼‰
    - è¾“å‡º: ä¸€ä¸ªè¿ç»­è´¨é‡åˆ†æ•° q âˆˆ [0, 1]ï¼Œç”± logits ç»è¿‡ sigmoid å¾—åˆ°
    """

    def __init__(
        self,
        model_name: str = "/root/autodl-tmp/EasyR1/ckpt/reward-model-deberta-v3-large-v2",
        use_bnb_4bit: bool = False,
        device_map: str = "auto",
        torch_dtype: torch.dtype = torch.bfloat16,
        max_length: int = 2048,
    ):
        self.model_name = model_name
        self.max_length = max_length

        if use_bnb_4bit:
            # å¯¹ encoder RM é€šå¸¸æ²¡å¿…è¦ 4bitï¼Œä½†å¦‚æœä½ ç¡¬ä»¶ç´§å¼ å¯ä»¥æ‰“å¼€
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
            )
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                trust_remote_code=True,
                device_map=device_map,
            )
        else:
            self.model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                trust_remote_code=True,
                device_map=device_map,
                torch_dtype=torch_dtype,
            )

        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

    def _build_input(self, question: Optional[str], answer_text: str) -> str:
        if question and question.strip():
            return f"Question: {question.strip()}\nAnswer: {answer_text.strip()}"
        else:
            return answer_text.strip()

    def score(self, question: Optional[str], answer_text: str) -> float:
        """
        è¿”å›è´¨é‡åˆ†æ•° (0~1)ã€‚å†…éƒ¨æ˜¯å¯¹ logits åš sigmoidã€‚
        æ³¨æ„ï¼šè¿™æ˜¯ç›¸å¯¹æ„ä¹‰çš„è´¨é‡å€¾å‘åˆ†æ•°ï¼Œå»ºè®®åªåœ¨åŒä¸€ prompt å¤šå€™é€‰ä¹‹é—´æ¯”è¾ƒæˆ–æ’åºã€‚
        """
        text = self._build_input(question, answer_text)
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=self.max_length,
            padding=False
        )
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        with torch.no_grad():
            logits = self.model(**inputs).logits.squeeze()
            # DeBERTa RM é€šå¸¸æ˜¯ä¸€ä¸ªæ ‡é‡ logits
            if logits.numel() != 1:
                # è‹¥æ˜¯å¤šç»´ï¼Œå–æœ€åä¸€ç»´æˆ–ç¬¬ä¸€ä¸ªæ ‡é‡ï¼Œè¿™é‡Œåšå…¼å®¹å¤„ç†
                logits = logits.view(-1)[0]
            q = torch.sigmoid(logits).item()

        # ç¨³å¥è£å‰ª
        q = float(max(0.0, min(1.0, q)))
        return q

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è¯„åˆ†æµæ°´çº¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_score(reward_input: Dict[str, Any], format_weight: float = 0.1) -> Dict[str, float]:
    """
    è®¡ç®—å¥–åŠ±åˆ†æ•° - Sequentialæ¨¡å¼ï¼Œå•ä¸ªæ ·æœ¬å¤„ç†
    Args:
        reward_input: å•ä¸ªæ ·æœ¬çš„è¾“å…¥å­—å…¸
        format_weight: æ ¼å¼æƒé‡
    Returns:
        å•ä¸ªæ ·æœ¬çš„åˆ†æ•°å­—å…¸
    """
    if not isinstance(reward_input, dict):
        raise ValueError("qa_mixup reward function requires reward_type=sequential")

    if not _initialized:
        raise RuntimeError(
            "Reward system not initialized. Call init_reward_system(...) once in this process first."
        )

    return compute_single_score(reward_input, format_weight)

def compute_single_score(reward_input: Dict[str, Any], format_weight: float = 0.1) -> Dict[str, float]:
    """
    - quality: ä½¿ç”¨ OpenAssistant/reward-model-deberta-v3-large-v2 è¾“å‡ºçš„ sigmoid(logits) âˆˆ [0,1]
    - format: ç”±å›ºå®šè§„åˆ™åˆ¤å®šï¼ˆ<think></think><answer></answer>ï¼‰
    - alignment: ç”± MLP åˆ†ç±»å™¨ï¼ˆ0/1/2ï¼‰æ˜ å°„å¾—åˆ†
    """
    global _rater, _alignment_clf, _weights

    w = RewardWeights(
        quality=_weights.quality,
        alignment=_weights.alignment,
        format=format_weight
    ).normalize()

    response = reward_input.get("response", "")
    question = reward_input.get("question", None)
    answer_part = extract_answer_content(response)

    # 1) è´¨é‡åˆ†ï¼ˆè¿ç»­å‹ 0~1ï¼‰
    try:
        q_score = _rater.score(question, answer_part)  # already in [0,1]
    except Exception as e:
        print(f"âš ï¸  è´¨é‡è¯„åˆ†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        q_score = 0.5

    # 2) æ ¼å¼åˆ†ï¼ˆ{0,1}ï¼‰
    f_score = format_reward(response)

    # 3) å¯¹é½åˆ†ï¼ˆç¦»æ•£æ˜ å°„ï¼‰
    try:
        a_score = alignment_reward(answer_part, _alignment_clf)
    except Exception as e:
        print(f"âš ï¸  å¯¹é½è¯„åˆ†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        a_score = 0.5

    overall = w.quality * q_score + w.format * f_score + w.alignment * a_score

    return {
        "overall": float(overall),
        "quality": float(q_score),
        "format": float(f_score),
        "alignment": float(a_score),
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ åˆå§‹åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_reward_system(
    *,
    # ä½¿ç”¨ OpenAssistant çš„ RM
    rm_model_name: str = "/root/autodl-tmp/EasyR1/ckpt/reward-model-deberta-v3-large-v2",
    # alignment MLP ckpt
    alignment_model_path: str = "/root/autodl-tmp/EasyR1/examples/reward_function/mlp_classifier.pt",
    # embedding (ç”¨äº alignment)
    embedding_model_name_or_path: str = "/root/autodl-tmp/EasyR1/ckpt/bge-m3",
    weights: RewardWeights = RewardWeights(),
    # RM èµ„æºè®¾ç½®
    use_bnb_4bit: bool = False,
    device_map: str = "auto",
    torch_dtype=torch.float32,
    max_length: int = 2048,
) -> None:
    """
    åˆå§‹åŒ–ï¼š
    - å¥–åŠ±æ¨¡å‹ï¼ˆDeBERTa RMï¼‰
    - å¯¹é½åˆ†ç±»å™¨ï¼ˆMLPï¼‰
    - embedding æ¨¡å‹ï¼ˆSentenceTransformerï¼‰
    - å½’ä¸€åŒ–æƒé‡
    """
    global _alignment_clf, _embedding_model, _rater, _weights, _initialized

    if _initialized:
        print(f"ğŸ”„ PID {os.getpid()}: Reward systemå·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡")
        return

    print(f"ğŸš€ PID {os.getpid()}: å¼€å§‹åˆå§‹åŒ– reward system...")

    try:
        # 1) åˆå§‹åŒ– Reward Model (DeBERTa)
        print(f"ğŸ”„ PID {os.getpid()}: æ­£åœ¨åŠ è½½ Reward Model: {rm_model_name} ...")
        _rater = RewardModelScorer(
            model_name=rm_model_name,
            use_bnb_4bit=use_bnb_4bit,
            device_map=device_map,
            torch_dtype=torch_dtype,
            max_length=max_length,
        )
        print(f"âœ… PID {os.getpid()}: Reward Model åŠ è½½å®Œæˆ")

        # 2) åˆå§‹åŒ–å¯¹é½åˆ†ç±»å™¨
        print(f"ğŸ”„ PID {os.getpid()}: æ­£åœ¨åŠ è½½å¯¹é½åˆ†ç±»å™¨: {alignment_model_path} ...")
        _alignment_clf = get_alignment_classifier(alignment_model_path)
        print(f"âœ… PID {os.getpid()}: å¯¹é½åˆ†ç±»å™¨åŠ è½½å®Œæˆ")

        # 3) åˆå§‹åŒ– embedding æ¨¡å‹
        print(f"ğŸ”„ PID {os.getpid()}: æ­£åœ¨åŠ è½½ embedding æ¨¡å‹: {embedding_model_name_or_path} ...")
        _embedding_model = SentenceTransformer(embedding_model_name_or_path)
        print(f"âœ… PID {os.getpid()}: embedding æ¨¡å‹åŠ è½½å®Œæˆ")

        _weights = weights.normalize()
        _initialized = True
        print(f"ğŸ‰ PID {os.getpid()}: Reward system åˆå§‹åŒ–å®Œå…¨å®Œæˆ!")
    except Exception as e:
        print(f"âŒ PID {os.getpid()}: Reward system åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    # åˆå§‹åŒ–ï¼ˆæ— é”ç‰ˆæœ¬ï¼‰
    init_reward_system(
        rm_model_name="OpenAssistant/reward-model-deberta-v3-large-v2",
        alignment_model_path="./models/alignment_mlp.pt",
        embedding_model_name_or_path="/root/autodl-tmp/EasyR1/ckpt/bge-m3",
        weights=RewardWeights(quality=0.6, alignment=0.3, format=0.1),
        use_bnb_4bit=False,  # ä¸€èˆ¬ä¸å»ºè®®å¯¹ encoder RM 4bitï¼Œé™¤éæ˜¾å­˜å¾ˆç´§
    )

    sample1 = {
        "response": "<think>some reasoning</think><answer>Paris is the capital of France.</answer>",
        "question": "What is the capital of France?",
        "answer": "The capital of France is Paris.",
    }
    print(compute_score(sample1, format_weight=0.1))

    sample2 = {
        "response": "bad format",
        "question": "Who wrote Hamlet?",
        "answer": "Shakespeare.",
        "rating_overall_compressed": 3,
    }
    print(compute_score(sample2, format_weight=0.1))